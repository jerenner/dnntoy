{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal vs. background classification in the double-escape peak\n",
    "\n",
    "In this notebook we read in the prepared data, construct and train the DNN, and then evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrenner/miniconda/envs/IC3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy  as np\n",
    "import random as rd\n",
    "import tables as tb\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib.patches         import Ellipse\n",
    "from __future__  import print_function\n",
    "\n",
    "# Keras imports\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.models               import Model, load_model\n",
    "from keras.layers               import Input, Dense, MaxPooling3D, AveragePooling3D, Conv3D, Conv2D, AveragePooling2D, Activation, Dropout, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers           import SGD, Adam, Nadam         \n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.core          import Flatten\n",
    "from keras                      import callbacks\n",
    "from keras.regularizers         import l2, l1\n",
    "from keras.initializers         import RandomNormal\n",
    "from keras.utils.layer_utils    import print_summary\n",
    "from keras                      import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable definitions\n",
    "Here we define key variables to be used throughout the notebook.  Note that we will read the data from a directory `data_location/run_name`, and it is stored in multiple files:\n",
    "- The training data will consist of the events stored in files from `train_fstart` to `train_fend`\n",
    "- The test data will consist of the events stored in files from `test_fstart` to `test_fend`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data dimensions\n",
    "xdim = 48\n",
    "ydim = 48\n",
    "zdim = 48\n",
    "\n",
    "# data location and training/test file numbers\n",
    "data_location = \"/data/fastmc/descape/classification\"\n",
    "run_name = \"data\"\n",
    "train_fstart = 0\n",
    "train_fend = 45\n",
    "test_fstart = 45\n",
    "test_fend = 50\n",
    "\n",
    "evt_limit = 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "\n",
    "### Data input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the function to read the data from multiple files\n",
    "def read_data(loc, rname, f_start, f_end, fval):\n",
    "    \"\"\"Reads all events from the files with the specified file numbers.\"\"\"\n",
    "    \n",
    "    # read in the signal events.\n",
    "    print(\"Reading signal events...\")\n",
    "    for fn in range(f_start,f_end):\n",
    "        s_dat = tb.open_file(\"{0}/{1}/vox_dnn_Xe_EPEM_7bar_descape_2x2x2_out_{2}.h5\".format(loc,rname,fn), 'r')\n",
    "        if(fn == f_start):\n",
    "            s_array = np.array(s_dat.root.maps)\n",
    "            s_energies = np.array(s_dat.root.energies)\n",
    "            print(\"-- Reading file {0},\".format(fn), end=' ')\n",
    "        else:\n",
    "            print(\"{0},\".format(fn), end=' ')\n",
    "            s_array = np.concatenate([s_array,np.array(s_dat.root.maps)])\n",
    "            s_energies = np.concatenate([s_energies,np.array(s_dat.root.energies)])\n",
    "    print(\"done.\")\n",
    "    if(evt_limit > 0):\n",
    "        s_array = s_array[0:evt_limit]\n",
    "        s_energies = s_energies[0:evt_limit]\n",
    "\n",
    "    # read in the background events.\n",
    "    print(\"\\nReading background events...\")\n",
    "    for fn in range(f_start,f_end):\n",
    "        b_dat = tb.open_file(\"{0}/{1}/vox_dnn_Xe_SE_7bar_descape_2x2x2_out_{2}.h5\".format(loc,rname,fn), 'r')\n",
    "        if(fn == f_start):\n",
    "            print(\"-- Reading file {0},\".format(fn), end=' ')\n",
    "            b_array = np.array(b_dat.root.maps)\n",
    "            b_energies = np.array(b_dat.root.energies)\n",
    "        else:\n",
    "            print(\"{0},\".format(fn), end=' ')\n",
    "            b_array = np.concatenate([b_array,np.array(b_dat.root.maps)])\n",
    "            b_energies = np.concatenate([b_energies,np.array(b_dat.root.energies)])\n",
    "    print(\"done.\")\n",
    "    if(evt_limit > 0):\n",
    "        b_array = b_array[0:evt_limit]\n",
    "        b_energies = b_energies[0:evt_limit]\n",
    "    print(\"\\nRead {0} signal events and {1} background events.\".format(len(s_array),len(b_array)))\n",
    "        \n",
    "    # concatenate the datasets, splitting into training and validation sets\n",
    "    print(\"Concatenating datasets...\")\n",
    "    nval = int(fval * (len(s_array) + len(b_array)))\n",
    "    \n",
    "    if(nval == 0):\n",
    "        x_ = np.concatenate([s_array, b_array])\n",
    "        y_ = np.concatenate([np.ones([len(s_array), 1]), np.zeros([len(b_array), 1])])\n",
    "\n",
    "        # reshape for training with TensorFlow        \n",
    "        print(\"Reshaping projection...\")\n",
    "        x_ = np.reshape(x_, (len(x_), xdim, ydim, zdim, 1))\n",
    "        print(\"Finished reading data: {0} training/test events\".format(len(x_)))\n",
    "        \n",
    "        #mval = np.mean(x_)\n",
    "        #sigval = np.std(x_)\n",
    "        #x_ -= mval\n",
    "        #x_ /= sigval\n",
    "        \n",
    "        return x_,y_\n",
    "    else:\n",
    "        x_ = np.concatenate([s_array[0:-nval], b_array[0:-nval]])\n",
    "        y_ = np.concatenate([np.ones([len(s_array[0:-nval]), 1]), np.zeros([len(b_array[0:-nval]), 1])])\n",
    "        xval_ = np.concatenate([s_array[-nval:], b_array[-nval:]])\n",
    "        yval_ = np.concatenate([np.ones([len(s_array[-nval:]), 1]), np.zeros([len(b_array[-nval:]), 1])])\n",
    "\n",
    "        # reshape for training with TensorFlow\n",
    "        print(\"Reshaping projection...\")\n",
    "        x_ = np.reshape(x_, (len(x_), xdim, ydim, zdim, 1))\n",
    "        xval_ = np.reshape(xval_, (len(xval_), xdim, ydim, zdim, 1))\n",
    "        print(\"Finished reading data: {0} training/test and {1} validation events\".format(len(x_),len(xval_)))\n",
    "        \n",
    "        #mval = np.mean(x_)\n",
    "        #sigval = np.std(x_)\n",
    "        #x_ -= mval\n",
    "        #x_ /= sigval\n",
    "        \n",
    "        #mval = np.mean(xval_)\n",
    "        #sigval = np.std(xval_)\n",
    "        #xval_ -= mval\n",
    "        #xval_ /= sigval\n",
    "        \n",
    "        \n",
    "        return x_,y_,xval_,yval_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the function to read the data from multiple files\n",
    "def read_real_data(fname):\n",
    "    \"\"\"Reads data from the specified file.\"\"\"\n",
    "    \n",
    "    # read in the signal events.\n",
    "    print(\"Reading real data...\")\n",
    "    f_dat = tb.open_file(fname, 'r')\n",
    "    f_array = np.array(f_dat.root.maps)\n",
    "    f_evts = np.array(f_dat.root.evtnum)\n",
    "    print(\"done.\")\n",
    "\n",
    "    # reshape for training with TensorFlow        \n",
    "    print(\"Reshaping...\")\n",
    "    x_ = np.reshape(f_array, (len(f_array), xdim, ydim, zdim, 1))\n",
    "    \n",
    "    #mval = np.mean(x_)\n",
    "    #sigval = np.std(x_)\n",
    "    \n",
    "    #x_ -= mval\n",
    "    #x_ /= sigval\n",
    "    \n",
    "    print(\"Finished reading data: {0} events\".format(len(x_)))\n",
    "    return x_,f_evts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network models\n",
    "These functions should define and return a Keras model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "# Define more neural networks here\n",
    "\n",
    "def model_3D(inputs):\n",
    "    \n",
    "    cinputs = Conv3D(64, (3, 3, 3), padding='valid', strides=(1, 1, 1), activation='relu',kernel_initializer='glorot_normal', kernel_regularizer=l2(0.000001))(inputs)\n",
    "    cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='same', data_format=None)(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-10, axis=-1, momentum=0.2, weights=None, gamma_regularizer=None, beta_regularizer=None, beta_initializer=\"zero\", gamma_initializer=\"one\")(cinputs)\n",
    "    cinputs = Conv3D(128, (3, 3, 3), padding='valid', strides=(1, 1, 1), activation='relu',kernel_initializer='glorot_normal', kernel_regularizer=l2(0.000001))(cinputs)\n",
    "    cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='same', data_format=None)(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=4, momentum=0.8, weights=None, gamma_regularizer=None, beta_regularizer=None, beta_initializer=\"zero\", gamma_initializer=\"one\")(cinputs)\n",
    "    cinputs = Conv3D(256, (2, 2, 2), padding='valid', strides=(1, 1, 1), activation='relu',kernel_initializer='glorot_normal', kernel_regularizer=l2(0.000001))(cinputs)\n",
    "    cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='same', data_format=None)(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=4, momentum=0.99, weights=None, gamma_regularizer=None, beta_regularizer=None, beta_initializer=\"zero\", gamma_initializer=\"one\")(cinputs)\n",
    "    cinputs = Conv3D(256, (2, 2, 2), padding='valid', strides=(1, 1, 1), activation='relu',kernel_initializer='glorot_normal', kernel_regularizer=l2(0.000001))(cinputs)\n",
    "    cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='same', data_format=None)(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=4, momentum=0.99, weights=None, gamma_regularizer=None, beta_regularizer=None, beta_initializer=\"zero\", gamma_initializer=\"one\")(cinputs)\n",
    "    cinputs = Conv3D(256, (2, 2, 2), padding='valid', strides=(1, 1, 1), activation='relu',kernel_initializer='glorot_normal', kernel_regularizer=l2(0.000001))(cinputs)\n",
    "    cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='same', data_format=None)(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=4, momentum=0.99, weights=None, gamma_regularizer=None, beta_regularizer=None, beta_initializer=\"zero\", gamma_initializer=\"one\")(cinputs)\n",
    "    f1 = Flatten()(cinputs)\n",
    "    f1 = Dense(units=512, activation='relu', kernel_initializer='glorot_normal', kernel_regularizer=l2(0.000001))(f1)\n",
    "    f1 = Dropout(.7)(f1)\n",
    "\n",
    "    inc_output = Dense(units=1, activation='sigmoid', kernel_initializer='normal', kernel_regularizer=l2(0.000001))(f1)\n",
    "    model = Model(inputs, inc_output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer=Nadam(lr=0.000002, beta_1=0.9, beta_2=0.999,\n",
    "                                epsilon=1e-08, schedule_decay=0.001), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_3D_old(inputs):\n",
    "    \n",
    "    cinputs = Conv3D(512, (10, 10, 10), padding='valid', strides=(2, 2, 2), activation='relu',kernel_initializer='lecun_uniform', kernel_regularizer=l2(0.000001))(inputs)\n",
    "    cinputs = MaxPooling3D(pool_size=(5, 5, 5), strides=(5, 5, 5), padding='same', data_format=None)(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=4, momentum=0.99, weights=None, gamma_regularizer=None, beta_regularizer=None, beta_initializer=\"zero\", gamma_initializer=\"one\")(cinputs)\n",
    "    cinputs = Conv3D(256, (1, 1, 1), padding='same', strides=(1, 1, 1), activation='relu',kernel_initializer='lecun_uniform', kernel_regularizer=l2(0.000001))(cinputs)\n",
    "    cinputs = Conv3D(128, (2, 2, 2), padding='same', strides=(2, 2, 2), activation='relu',kernel_initializer='lecun_uniform', kernel_regularizer=l2(0.000001))(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=4, momentum=0.99, weights=None, gamma_regularizer=None, beta_regularizer=None, beta_initializer=\"zero\", gamma_initializer=\"one\")(cinputs)\n",
    "    cinputs = Conv3D(256, (2, 2, 2), padding='same', strides=(2, 2, 2), activation='relu',kernel_initializer='lecun_uniform', kernel_regularizer=l2(0.000001))(cinputs)\n",
    "    cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='same', data_format=None)(cinputs)\n",
    "    f1 = Flatten()(cinputs)\n",
    "    f1 = Dense(units=128, activation='relu', kernel_initializer='lecun_uniform', kernel_regularizer=l2(0.000001))(f1)\n",
    "    f1 = Dropout(.3)(f1)\n",
    "\n",
    "    inc_output = Dense(units=1, activation='sigmoid', kernel_initializer='normal', kernel_regularizer=l2(0.000001))(f1)\n",
    "    model = Model(inputs, inc_output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer=Nadam(lr=0.00001, beta_1=0.9, beta_2=0.999,\n",
    "                                epsilon=1e-08, schedule_decay=0.01), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_CNN(inputs):\n",
    "#, kernel_regularizer=l2(0.1)    \n",
    "    cinputs = Conv2D(32, (5, 5), padding='same', strides=(2, 2), activation='relu', kernel_initializer='he_normal')(inputs)\n",
    "    cinputs = AveragePooling2D(pool_size=(2, 2), data_format=None, padding=\"same\", strides=(2, 2))(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=3, momentum=0.99, weights=None, beta_initializer='zero', gamma_initializer='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "    cinputs = Conv2D(64, (3, 3), padding='same', strides=(1, 1), activation='relu', kernel_initializer='he_normal')(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=3, momentum=0.99, weights=None, beta_initializer='zero', gamma_initializer='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "    cinputs = AveragePooling2D(pool_size=(2, 2), data_format=None, padding=\"same\", strides=(2, 2))(cinputs)\n",
    "    cinputs = Conv2D(128, (2, 2), padding='same', strides=(1, 1), activation='relu', kernel_initializer='he_normal')(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=3, momentum=0.99, weights=None, beta_initializer='zero', gamma_initializer='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "    cinputs = AveragePooling2D(pool_size=(2, 2), data_format=None, padding=\"same\", strides=(2, 2))(cinputs)\n",
    "    f1 = Flatten()(cinputs)\n",
    "    f1 = Dense(units=16, activation='relu', kernel_initializer='he_normal')(f1)\n",
    "    f1 = Dropout(.4)(f1)\n",
    "\n",
    "    inc_output = Dense(units=1, activation='sigmoid', kernel_initializer='lecun_normal')(f1)\n",
    "    model = Model(inputs, inc_output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Nadam(lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                                      epsilon=1e-08, schedule_decay=0.001), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testmodel\n",
    "def model_test(inpus):\n",
    "    f1 = Flatten()(inputs)\n",
    "    f1 = Dense(units=128, kernel_initializer=\"normal\", activation=\"relu\", kernel_regularizer=l2(0.00001), activity_regularizer=l1(0.00001))(f1)\n",
    "    f1 = Dropout(.7)(f1)\n",
    "    inc_output = Dense(units=1, kernel_initializer=\"normal\", activation=\"sigmoid\")(f1)\n",
    "    model = Model(inputs, inc_output)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004), metrics=['accuracy'])  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a fully-connected neural network with 64 hidden neurons and 1 readout neuron\n",
    "def model_FC(inputs):\n",
    "    \n",
    "    f1 = Flatten()(inputs)\n",
    "    f1 = Dense(units=64, kernel_initializer=\"normal\", activation=\"sigmoid\")(f1)\n",
    "    f1 = Dense(units=64, kernel_initializer=\"normal\", activation=\"sigmoid\")(f1)\n",
    "    f1 = Dense(units=64, kernel_initializer=\"normal\", activation=\"sigmoid\")(f1)\n",
    "    f1 = Dropout(.3)(f1)\n",
    "    inc_output = Dense(units=1, kernel_initializer=\"normal\", activation=\"sigmoid\")(f1)\n",
    "    model = Model(inputs, inc_output)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Nadam(lr=0.0001, beta_1=0.9, beta_2=0.999,\n",
    "                                  epsilon=1e-08, schedule_decay=0.01),\n",
    "                                  metrics=['accuracy'])  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot a 20x20 SiPM map\n",
    "# -- carried over from NEW_kr_diff_mc_train.ipynb\n",
    "def NEW_SiPM_map_plot(xarr, normalize=True):\n",
    "    \"\"\"\n",
    "    Plots a SiPM map in the NEW Geometry\n",
    "    xarr is a NEW sipm map, yarr the pair of coordinates the map corresponds to\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        probs = (xarr - np.min(xarr))\n",
    "        probs /= np.max(probs)\n",
    "    else: \n",
    "        probs = xarr\n",
    "\n",
    "    # set up the figure\n",
    "    fig = plt.figure();\n",
    "    ax1 = fig.add_subplot(111);\n",
    "    fig.set_figheight(10.0)\n",
    "    fig.set_figwidth(10.0)\n",
    "    ax1.axis([0, 500, 0, 500]);\n",
    "\n",
    "    for i in range(xdim):\n",
    "        for j in range(ydim):\n",
    "            r = Ellipse(xy=(i * 10 + 5, j * 10 + 5), width=5., height=5.);\n",
    "            r.set_facecolor('0');\n",
    "            r.set_alpha(probs[i, j]);\n",
    "            ax1.add_artist(r);\n",
    "        \n",
    "    plt.xlabel(\"x (mm)\");\n",
    "    plt.ylabel(\"y (mm)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to fix image writing in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tboard_fix(tbrd, model):\n",
    "        sess = K.get_session()\n",
    "        for layer in model.layers:\n",
    "\n",
    "            for weight in layer.weights:\n",
    "                tf.summary.histogram(weight.name, weight)\n",
    "                shape = weight.get_shape()\n",
    "                if not(len(shape) == 1 and shape[0] == 1):\n",
    "                    w_img = tf.squeeze(weight)\n",
    "                    shape = w_img.get_shape()\n",
    "                    print(\"Shape is {0}\".format(shape))\n",
    "                    if len(shape) > 1 and shape[0] > shape[1]:\n",
    "                        w_img = tf.transpose(w_img)\n",
    "                    if len(shape) == 1:\n",
    "                        w_img = tf.expand_dims(tf.expand_dims(tf.expand_dims(w_img,0), 0), -1)\n",
    "                    if len(shape) == 2:\n",
    "                        w_img = tf.expand_dims(tf.expand_dims(w_img, 0), -1)\n",
    "                    if len(shape) == 3:\n",
    "                        w_img = tf.transpose(w_img, perm=[2, 0, 1])\n",
    "                        w_img = tf.expand_dims(w_img, -1)\n",
    "                    if(len(shape) == 4):\n",
    "                        for ii in range(shape[3]):\n",
    "                            print(\"-- Adding image for filter {0}\".format(ii))\n",
    "                            wi_img = tf.transpose(w_img[:,:,:,ii], perm=[2, 0, 1])\n",
    "                            wi_img = tf.expand_dims(wi_img,-1)\n",
    "                            tf.summary.image(\"{0}_{1}\".format(weight.name,ii),wi_img)\n",
    "                    elif(len(shape) < 5):\n",
    "                        tf.summary.image(weight.name, w_img)\n",
    "\n",
    "            if hasattr(layer, 'output'):\n",
    "                tf.summary.histogram('{}_out'.format(layer.name),\n",
    "                                     layer.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading signal events...\n",
      "-- Reading file 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, "
     ]
    }
   ],
   "source": [
    "# read in the training data\n",
    "x_train, y_train, x_val, y_val = read_data(data_location, run_name, train_fstart, train_fend, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one slice of one event\n",
    "NEW_SiPM_map_plot(x_train[2,:,:,0,0])\n",
    "print(np.sum(x_train[0,:,:,:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train the DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set load_model to true and specify the file to load in a previously defined/trained model\n",
    "load_weights = False\n",
    "mfile = 'models/weights-07-0.5093-0.5288.h5'\n",
    "\n",
    "if(load_weights):\n",
    "    model = load_model(mfile)\n",
    "else:\n",
    "    \n",
    "    # otherwise define the model\n",
    "    inputs = Input(shape=(xdim, ydim, zdim, 1))\n",
    "    model = model_3D(inputs)\n",
    "    \n",
    "# define callbacks (actions to be taken after each epoch of training)\n",
    "file_lbl = \"{epoch:02d}-{loss:.4f}-{val_loss:.4f}\"\n",
    "filepath=\"weights-{0}.h5\".format(file_lbl)\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "tboard = callbacks.TensorBoard(log_dir='/data/fastmc/descape/logs', histogram_freq=0, write_graph=True, write_images=False)\n",
    "#tboard_fix(tboard,model)\n",
    "lcallbacks = [checkpoint, tboard]\n",
    "model.summary()\n",
    "\n",
    "#for layer in model.layers:\n",
    "#    cls = layer.__class__.__name__\n",
    "#    if(cls == \"Conv3D\"):\n",
    "#        print(\"{0} ({1}) kernel = {2}; filters = {3}; strides = {4}; activation = {5}\".format(cls,layer.name,layer.kernel_size,layer.filters,layer.strides,layer.activation))\n",
    "#    elif(cls == \"MaxPooling3D\"):\n",
    "#        print(\"{0} ({1}) pool = {2}; strides = {3}\".format(cls,layer.name,layer.pool_size,layer.strides))\n",
    "#    elif(cls == \"BatchNormalization\"):\n",
    "#        print(\"{0} ({1})\".format(cls,layer.name))\n",
    "#    elif(cls == \"Dense\"):\n",
    "#        print(\"{0} ({1}) units = {2}; activation = {3}\".format(cls,layer.name,layer.units,layer.activation))\n",
    "#    elif(cls == \"Dropout\"):\n",
    "#        print(\"{0} ({1}) rate = {2}\".format(cls,layer.name,layer.rate))\n",
    "#    else:\n",
    "#        print(\"{0} ({1})\".format(cls,layer.name))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      " 1000/27000 [>.............................] - ETA: 2:15 - loss: 0.2174 - acc: 0.9190"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f0a85ea03fbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda/envs/IC3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda/envs/IC3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/IC3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/IC3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/IC3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/IC3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/IC3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/IC3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "hist = model.fit(x_train, y_train, shuffle=True, epochs=100, batch_size=50, verbose=1, validation_data=(x_val,y_val), callbacks=lcallbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the predictions for real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_real, evt_real = read_real_data(\"/data/fastmc/descape/classification/data_4735_nocenter_E.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_real = model.predict(x_real, batch_size=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npred_signal = sum(y_real > 0.205)\n",
    "npred_background = sum(y_real <= 0.205)\n",
    "print(\"Number of predicted signal events = {}, background events = {}\".format(npred_signal,npred_background))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the arrays to a file.\n",
    "ysave = y_real.reshape(len(y_real))\n",
    "esave = evt_real.reshape(len(evt_real))\n",
    "np.savez(\"classification.npz\",evtnum=esave,y=ysave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(x_real[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the test data\n",
    "x_test, y_test = read_data(data_location, run_name, test_fstart, test_fend, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the predictions\n",
    "loss_and_metrics = model.evaluate(x_test, y_test);\n",
    "y_pred = model.predict(x_test, batch_size=50, verbose=0)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create lists of values for signal vs. background curve\n",
    "npoints = 200\n",
    "fname_svsb = \"plt/plt_convnet_4M_gabriel_pRelu.h5\"\n",
    "bg_rej = []; si_eff = []\n",
    "print(\"-- Calculating points...\")\n",
    "for thh in np.arange(0,1,1./npoints):\n",
    "    nts = 0; ntb = 0\n",
    "    ncs = 0; ncb = 0\n",
    "    for ye,yp in zip(y_test,y_pred):\n",
    "        if(ye == 0):\n",
    "            ntb += 1  # add one background event\n",
    "            if(yp < thh):\n",
    "                ncb += 1  # add one correctly predicted background event\n",
    "\n",
    "        if(ye == 1):\n",
    "            nts += 1  # add one signal event\n",
    "            if(yp >= thh):\n",
    "                ncs += 1  # add one correctly predicted signal event\n",
    "                \n",
    "    si_eff.append(1.0*ncs/nts)\n",
    "    bg_rej.append(1.0*ncb/ntb)\n",
    "    #print(\"-- {0} of {1} ({2}%) correct background events; {3} of {4} ({5}%) correct signal events\".format(ncb,ntb,1.0*ncb/ntb*100,ncs,nts,1.0*ncs/nts*100))\n",
    "\n",
    "# save the results to file\n",
    "print(\"-- Saving results...\")\n",
    "si_eff = np.array(si_eff); bg_rej = np.array(bg_rej)\n",
    "f = tb.open_file(fname_svsb, 'w')\n",
    "filters = tb.Filters(complib='blosc', complevel=9, shuffle=False)\n",
    "\n",
    "atom    = tb.Atom.from_dtype(si_eff.dtype)\n",
    "sarr    = f.create_earray(f.root, 'si_eff', atom, (0, npoints), filters=filters)\n",
    "sarr.append([si_eff])\n",
    "\n",
    "atom    = tb.Atom.from_dtype(bg_rej.dtype)\n",
    "sarr    = f.create_earray(f.root, 'bg_rej', atom, (0, npoints), filters=filters)\n",
    "sarr.append([bg_rej])\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot signal vs. background curves\n",
    "#fnames = [\"plt/reset_V10_iter100.h5\", \"plt/reset_V10_iter500.h5\", \"plt/reset_V10_iter1000.h5\", \"plt/reset_V10_2D.h5\", \"plt/classification_V10_3Dconv.h5\"]\n",
    "#labels = [\"RESET V10, iter100\", \"RESET V10, iter500\", \"RESET V10, iter1000\", \"RESET V10, latest\", \"SiPMs only (3D), V10\"]\n",
    "#colors = [\"green\", \"black\", \"brown\", \"blue\", \"orange\"]\n",
    "fnames = [\"plt/plt_conv3d_classifier_300317.h5\", \"plt/plt_convnet_4M_gabriel.h5\", \"plt/plt_convnet_4M_gabriel_pRelu.h5\"]\n",
    "labels = [\"Previous Result\", \"CONVNET 4M\", \"CONVNET 4M PRELU\"]\n",
    "colors = [\"yellow\", \"blue\", \"red\"]\n",
    "\n",
    "# set up the plot\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(5.0)\n",
    "fig.set_figwidth(7.5)\n",
    "\n",
    "for nm,lb,co in zip(fnames,labels,colors):\n",
    "    \n",
    "    # read in the signal efficiency vs. background rejection information\n",
    "    fn = tb.open_file(nm,'r')\n",
    "    eff = fn.root.si_eff[0]\n",
    "    bgr = fn.root.bg_rej[0]\n",
    "    \n",
    "    plt.plot(eff,bgr,color=co,label=lb,lw=2)\n",
    "    fn.close()\n",
    "    \n",
    "plt.xlabel(\"signal efficiency\")\n",
    "plt.ylabel(\"background rejection\")\n",
    "plt.legend(loc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Compare data and MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_MC_si = x_train[0:998]\n",
    "x_MC_bg = x_train[17100:18098]\n",
    "x_data = x_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(x_MC_bg[3]))\n",
    "print(np.std(x_MC_bg[12]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
